# Deep Q-Networks (DQN)

## What is a DQN?

A Deep Q-Network (DQN) is a reinforcement learning algorithm that combines Q-learning with deep neural networks to learn optimal decision-making policies in environments with high-dimensional state spaces.

In classical Q-learning, an agent maintains a Q-table that maps every (state, action) pair to an expected cumulative reward. This becomes infeasible when the state space is large or continuous (e.g., raw pixel inputs from a video game). DQN replaces the Q-table with a deep neural network that approximates the Q-function Q(s, a), taking a state as input and outputting estimated action values for all possible actions.

> The "Q" stands for Quality. Specifically, Q(s, a) represents the quality of taking action a in state s, measured as the expected cumulative discounted reward the agent will receive from that point onward if it follows the optimal policy. Higher Q-values indicate more desirable state-action pairs.

The algorithm was introduced by DeepMind in 2013 and refined in a landmark 2015 Nature paper, where it achieved human-level performance on a wide range of Atari 2600 games using only raw pixel observations and game scores as input.

![Deep-Q Network](https://logus2k.com/docbro/categories/rl/images/dqn.png)

## Core Components

### Q-Function Approximation

A neural network (typically a CNN for image-based inputs) parameterized by weights $\theta$ approximates:

$$Q(s, a; \theta) \approx Q^*(s, a)$$

where $Q^*$ is the optimal action-value function. The network takes a state $s$ and outputs a vector of Q-values, one per discrete action.

### Experience Replay

Transitions $(s, a, r, s', \text{done})$ are stored in a fixed-size replay buffer. During training, random mini-batches are sampled from this buffer. This mechanism serves two purposes: it breaks temporal correlations between consecutive samples, and it allows each experience to be reused multiple times, improving sample efficiency.

### Target Network

A separate copy of the Q-network, called the target network (parameterized by $\theta^-$), is used to compute target Q-values. Its weights are copied from the main network at fixed intervals (e.g., every $N$ steps). This provides stable targets during training and prevents the instability caused by the network chasing its own moving predictions.

### Training Objective

The network is trained by minimizing the temporal difference (TD) loss:

$$\mathcal{L}(\theta) = \mathbb{E}\left[\left(r + \gamma \cdot \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta)\right)^2\right]$$

where $\gamma$ is the discount factor and the expectation is over sampled mini-batches from the replay buffer.

### Action Selection

DQN uses an epsilon-greedy policy: with probability $\varepsilon$ the agent takes a random action (exploration), and with probability $1 - \varepsilon$ it selects the action with the highest Q-value (exploitation). Epsilon is typically annealed from a high value (e.g., $1.0$) to a low value (e.g., $0.01$) over the course of training.

## Training Loop

1. Observe current state $s$.
2. Select action $a$ via epsilon-greedy on $Q(s, \cdot\,; \theta)$.
3. Execute action $a$, observe reward $r$ and next state $s'$.
4. Store transition $(s, a, r, s', \text{done})$ in the replay buffer.
5. Sample a random mini-batch from the replay buffer.
6. Compute targets: $y = r + \gamma \cdot \max_{a'} Q(s', a'; \theta^-)$ (or $y = r$ if terminal).
7. Update $\theta$ by gradient descent on $(y - Q(s, a; \theta))^2$.
8. Every $C$ steps, copy $\theta \rightarrow \theta^-$ (update target network).

## Advantages

**Handles high-dimensional state spaces.** By using neural networks as function approximators, DQN can learn directly from raw sensory inputs like images, eliminating the need for hand-crafted feature engineering.

**Sample reuse through experience replay.** Each transition can be used for multiple gradient updates, making the algorithm significantly more sample-efficient than on-policy methods.

**Stable training dynamics.** The target network and experience replay together address two major sources of instability in combining neural networks with Q-learning: correlated samples and non-stationary targets.

**End-to-end learning.** The entire pipeline from perception to decision-making is learned jointly, allowing the network to discover task-relevant features automatically.

**Off-policy learning.** DQN can learn from data generated by any policy (including past versions of itself or human demonstrations), which provides flexibility in data collection.

## Limitations

**Restricted to discrete action spaces.** The architecture requires computing Q-values for every possible action, making it inapplicable to continuous control tasks without modification.

**Q-value overestimation.** The max operator in the target computation introduces a systematic positive bias, leading to overoptimistic value estimates that can degrade policy quality.

**Poor exploration.** Epsilon-greedy is a naive exploration strategy that does not account for uncertainty or novelty, which leads to inefficient learning in environments with sparse or deceptive rewards.

**High memory and compute cost.** The replay buffer can consume significant memory, and training deep networks on large state spaces requires substantial GPU resources.

**Catastrophic forgetting.** The network may forget how to handle earlier situations as it trains on newer experiences, particularly in environments where the state distribution shifts over time.

**Sample inefficiency compared to model-based methods.** Despite replay, DQN still requires millions of interactions in complex environments, which is orders of magnitude more than model-based approaches.

## Notable Extensions and Variants

### Double DQN (DDQN)

Addresses Q-value overestimation by decoupling action selection from value estimation. The main network selects the best action, and the target network evaluates it:

$$y = r + \gamma \cdot Q\!\left(s',\; \underset{a'}{\arg\max}\; Q(s', a'; \theta);\; \theta^-\right)$$

This simple change significantly reduces overestimation bias and improves performance.

### Dueling DQN

Splits the network architecture into two streams: one estimates the state value $V(s)$, and the other estimates the advantage $A(s, a)$ of each action relative to the average. These are combined as:

$$Q(s, a) = V(s) + A(s, a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s, a')$$

This decomposition allows the network to learn which states are valuable without needing to evaluate every action, which is especially beneficial in states where the choice of action has little impact.

### Prioritized Experience Replay (PER)

Replaces uniform random sampling from the replay buffer with prioritized sampling based on TD error magnitude. Transitions with higher prediction error are sampled more frequently, focusing learning on the most informative experiences. Importance sampling weights are used to correct for the introduced bias.

### Noisy Networks (NoisyNet)

Replaces epsilon-greedy exploration with learned parametric noise added to the network weights. The noise parameters are trained alongside the network, enabling the agent to learn a state-dependent exploration strategy rather than relying on uniform random actions.

### Distributional DQN (C51)

Instead of estimating the expected Q-value, the network learns the full distribution of returns. The value distribution is modeled as a categorical distribution over a discrete set of support atoms. This richer signal provides more stable gradients and improves learning.

### Rainbow

Combines six orthogonal improvements into a single agent: Double DQN, Prioritized Experience Replay, Dueling architecture, Multi-step returns, Distributional RL (C51), and Noisy Networks. The combined agent significantly outperforms any individual component, demonstrating that these improvements are largely complementary.

### Other Extensions

**Multi-step returns (n-step DQN):** Uses n-step bootstrapped returns instead of single-step TD targets, providing a trade-off between bias and variance in value estimation.

**Hindsight Experience Replay (HER):** Augments the replay buffer with relabeled goals, enabling learning in sparse reward settings by treating failed episodes as successful attempts at alternative goals.

**Recurrent DQN (DRQN):** Replaces the feed-forward network with an LSTM to handle partially observable environments where the current observation alone is insufficient.

## When to Use DQN

DQN is a strong baseline choice when working with discrete action spaces, especially when the state space is high-dimensional (images, complex observations). For continuous action spaces, actor-critic methods like DDPG, TD3, or SAC are more appropriate. For environments requiring high sample efficiency, model-based methods should be considered. For multi-agent or cooperative settings, specialized extensions exist but vanilla DQN is typically insufficient.

## References

- Mnih et al. (2013). "Playing Atari with Deep Reinforcement Learning." arXiv:1312.5602.
- Mnih et al. (2015). "Human-level control through deep reinforcement learning." Nature, 518(7540).
- van Hasselt et al. (2016). "Deep Reinforcement Learning with Double Q-learning." AAAI.
- Wang et al. (2016). "Dueling Network Architectures for Deep Reinforcement Learning." ICML.
- Schaul et al. (2016). "Prioritized Experience Replay." ICLR.
- Bellemare et al. (2017). "A Distributional Perspective on Reinforcement Learning." ICML.
- Hessel et al. (2018). "Rainbow: Combining Improvements in Deep Reinforcement Learning." AAAI.
