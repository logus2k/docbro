# Representation Techniques

| Order | Technique | Year | Description | Popular Models/Frameworks | Pros | Cons |
|-------|-----------|------|-------------|---------------------------|------|------|
| 1 | Bag of Words (BoW) | 1950s-1960s | Represents text as unordered collection of word counts, ignoring grammar and word order | scikit-learn CountVectorizer, NLTK | Simple to implement, computationally efficient, good baseline for text classification | Ignores word order and context, high dimensionality, no semantic meaning, sparse representations |
| 2 | Co-occurrence matrices | 1980s-1990s | Counts how often words appear together within context windows to capture semantic relationships | Traditional NLP libraries, PMI | Captures word relationships, basis for semantic similarity, handles synonymy well | Very high dimensionality, sparse matrices, computationally expensive, memory intensive |
| 3 | One-Hot Encoding | 1990s-2000s | Creates binary vectors where each word gets a unique position, representing categorical variables | pandas get_dummies, scikit-learn OneHotEncoder | Simple implementation, preserves categorical nature, no assumptions about relationships | Extremely high dimensionality, sparse vectors, no semantic relationships, curse of dimensionality |
| 4 | Word Embeddings | 2003 (Word2Vec 2013, GloVe 2014) | Dense vector representations that capture semantic meaning and relationships between words | Word2Vec, GloVe, FastText, gensim | Dense representations, captures semantic meaning, handles synonyms, computationally efficient | Fixed vocabulary, out-of-vocabulary problems, context-independent (static) |
| 5 | Transformers | 2017 | Attention-based architecture that processes all words simultaneously, capturing context more effectively | BERT, GPT series, T5, Hugging Face Transformers | Contextual representations, handles long-range dependencies, state-of-the-art performance, handles polysemy | Computationally expensive, requires large datasets, memory intensive, complex to train |
